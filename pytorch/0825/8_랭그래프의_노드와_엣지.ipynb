{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 랭그래프의 노드를 이루는 요소\n",
        "\n",
        "- Agent : LLM을 사용해서 동작에 대한 판단을 스스로 내리는 AI 구성요소\n",
        "\n",
        "- Tool : 단순 함수 Call"
      ],
      "metadata": {
        "id": "JQHpdJTv91tn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzOD_QGR9kfk",
        "outputId": "da744bcd-8d1c-4435-9228-6fd34a762da8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.100.0)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.74)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.14)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain openai langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, openai\n",
        "from google.colab import userdata\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get(\"sesac7_openAI2\")"
      ],
      "metadata": {
        "id": "wGuAWZ2S_A5d"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tempfile import tempdir\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "# 4.74\n",
        "print(llm.predict('what is 2.1^2.1?'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mioAEHF_Uk_",
        "outputId": "5a29e687-0721-42b9-c7a8-6ccc05e967ee"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:OpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"what is 2.1^2.1?\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:OpenAI] [531ms] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"\\n\\nThe answer is approximately 4.41.\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"Generation\"\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 10,\n",
            "      \"prompt_tokens\": 11,\n",
            "      \"total_tokens\": 21\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo-instruct\"\n",
            "  },\n",
            "  \"run\": null,\n",
            "  \"type\": \"LLMResult\"\n",
            "}\n",
            "\n",
            "\n",
            "The answer is approximately 4.41.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import Tool\n",
        "from langchain.chains import LLMMathChain\n",
        "\n",
        "# 동일한 세팅의 llm을 LLMMathChain에게 전달\n",
        "llm_math = LLMMathChain(llm=OpenAI(temperature=0))\n",
        "\n",
        "# 수학적 툴\n",
        "math_tool = Tool(\n",
        "    name = 'calculator',\n",
        "    func = llm_math.run,\n",
        "    description = '수학 계산과 관련된 문제에 대해 필요하면 이 도구를 쓰시오.'\n",
        ")\n",
        "\n",
        "tools = [math_tool]\n",
        "print(tools[0].name, tools[0].description)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6ETLLDQ_uyY",
        "outputId": "0e316ceb-5052-4ae1-c547-0ee6e04c2aea"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "calculator 수학 계산과 관련된 문제에 대해 필요하면 이 도구를 쓰시오.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1381266621.py:5: UserWarning: Directly instantiating an LLMMathChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
            "  llm_math = LLMMathChain(llm=OpenAI(temperature=0))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 위의 tool을 llm에 붙여 사용할 수 있게\n",
        "!pip install langgraph langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0rn6VqcASDh",
        "outputId": "d71b4f19-3f69-4e6b-f406-eeaa8723c99c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (0.6.6)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (0.3.31)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.74)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.1.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.7.0,>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.6.4)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.2.3)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.11.7)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.99.9 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.100.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.11.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (0.4.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (25.0)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph) (1.10.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.99.9->langchain-openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (0.24.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# agents \"판단\" / 실행을 하지는 않음\n",
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "import langchain\n",
        "\n",
        "# model(생각할 llm, agent의 두뇌),\n",
        "# tools(agent가 사용가능한 도규 목록),\n",
        "# prompt(agent가 판단할 때 어떤 기준이 있다면? 이걸 프롬프트가 담고 있다.)\n",
        "model = ChatOpenAI(model='gpt-3.5-turbo')\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    ('system', '너는 사용자를 도와주는 훌륭한 AI 어시스턴트야.'),\n",
        "    ('human', '{input}'),\n",
        "    ('placeholder', '{agent_scratchpad}')   # tools을 사용하면서 남기는 중간 작업 내역\n",
        "])\n",
        "\n",
        "# 디버깅 모드 활성화\n",
        "# langchain.debug = True\n",
        "\n",
        "# agent를 만듦 => 이 agent는 tool을 calling =>\n",
        "# model(두뇌)로 tools(도구)를 prompt(지시사항)에 맞게 판단해서 부름\n",
        "agent = create_tool_calling_agent(model, tools, prompt)\n",
        "\n",
        "# 에이전트의 실행\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
        "agent_executor.invoke({'input':'what is 2.1^2.1'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQw-T3H4Byfg",
        "outputId": "f1d5a0b1-f01d-44d2-bdd3-3d2958040777"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"what is 2.1^2.1\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad>] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad>] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad> > chain:RunnableLambda] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad> > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": []\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad>] [3ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"agent_scratchpad\": []\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad>] [4ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"input\": \"what is 2.1^2.1\",\n",
            "  \"intermediate_steps\": [],\n",
            "  \"agent_scratchpad\": []\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"what is 2.1^2.1\",\n",
            "  \"intermediate_steps\": [],\n",
            "  \"agent_scratchpad\": []\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
            "\u001b[0m[outputs]\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"System: 너는 사용자를 도와주는 훌륭한 AI 어시스턴트야.\\nHuman: what is 2.1^2.1\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > llm:ChatOpenAI] [606ms] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"tool_calls\",\n",
            "          \"model_name\": \"gpt-3.5-turbo-0125\",\n",
            "          \"service_tier\": \"default\"\n",
            "        },\n",
            "        \"type\": \"ChatGenerationChunk\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessageChunk\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"\",\n",
            "            \"additional_kwargs\": {\n",
            "              \"tool_calls\": [\n",
            "                {\n",
            "                  \"index\": 0,\n",
            "                  \"id\": \"call_Nsww8gzDlOnqZuI3n3soaVK1\",\n",
            "                  \"function\": {\n",
            "                    \"arguments\": \"{\\\"__arg1\\\":\\\"2.1^2.1\\\"}\",\n",
            "                    \"name\": \"calculator\"\n",
            "                  },\n",
            "                  \"type\": \"function\"\n",
            "                }\n",
            "              ]\n",
            "            },\n",
            "            \"response_metadata\": {\n",
            "              \"finish_reason\": \"tool_calls\",\n",
            "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
            "              \"service_tier\": \"default\"\n",
            "            },\n",
            "            \"type\": \"AIMessageChunk\",\n",
            "            \"id\": \"run--1bfd6232-9f82-414f-ac91-cd3ad9c4f652\",\n",
            "            \"tool_calls\": [\n",
            "              {\n",
            "                \"name\": \"calculator\",\n",
            "                \"args\": {\n",
            "                  \"__arg1\": \"2.1^2.1\"\n",
            "                },\n",
            "                \"id\": \"call_Nsww8gzDlOnqZuI3n3soaVK1\",\n",
            "                \"type\": \"tool_call\"\n",
            "              }\n",
            "            ],\n",
            "            \"tool_call_chunks\": [\n",
            "              {\n",
            "                \"name\": \"calculator\",\n",
            "                \"args\": \"{\\\"__arg1\\\":\\\"2.1^2.1\\\"}\",\n",
            "                \"id\": \"call_Nsww8gzDlOnqZuI3n3soaVK1\",\n",
            "                \"index\": 0,\n",
            "                \"type\": \"tool_call_chunk\"\n",
            "              }\n",
            "            ],\n",
            "            \"invalid_tool_calls\": []\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": null,\n",
            "  \"run\": null,\n",
            "  \"type\": \"LLMResult\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > parser:ToolsAgentOutputParser] Entering Parser run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > parser:ToolsAgentOutputParser] [1ms] Exiting Parser run with output:\n",
            "\u001b[0m[outputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence] [617ms] Exiting Chain run with output:\n",
            "\u001b[0m[outputs]\n",
            "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:calculator] Entering Tool run with input:\n",
            "\u001b[0m\"2.1^2.1\"\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:calculator > chain:LLMMathChain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"2.1^2.1\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:calculator > chain:LLMMathChain > chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"2.1^2.1\",\n",
            "  \"stop\": [\n",
            "    \"```output\"\n",
            "  ]\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:calculator > chain:LLMMathChain > chain:LLMChain > llm:OpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${Question with math problem.}\\n```text\\n${single line mathematical expression that solves the problem}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${Output of running the code}\\n```\\nAnswer: ${Answer}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\\\"37593 * 67\\\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\\\"37593**(1/5)\\\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: 2.1^2.1\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:calculator > chain:LLMMathChain > chain:LLMChain > llm:OpenAI] [937ms] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"```text\\n2.1**2.1\\n```\\n...numexpr.evaluate(\\\"2.1**2.1\\\")...\\n\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"Generation\"\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 27,\n",
            "      \"prompt_tokens\": 200,\n",
            "      \"total_tokens\": 227\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo-instruct\"\n",
            "  },\n",
            "  \"run\": null,\n",
            "  \"type\": \"LLMResult\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:calculator > chain:LLMMathChain > chain:LLMChain] [947ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"text\": \"```text\\n2.1**2.1\\n```\\n...numexpr.evaluate(\\\"2.1**2.1\\\")...\\n\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:calculator > chain:LLMMathChain] [948ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"answer\": \"Answer: 4.749638091742242\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:calculator] [949ms] Exiting Tool run with output:\n",
            "\u001b[0m\"Answer: 4.749638091742242\"\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad>] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad>] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad> > chain:RunnableLambda] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad> > chain:RunnableLambda] [7ms] Exiting Chain run with output:\n",
            "\u001b[0m[outputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad>] [15ms] Exiting Chain run with output:\n",
            "\u001b[0m[outputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad>] [24ms] Exiting Chain run with output:\n",
            "\u001b[0m[outputs]\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
            "\u001b[0m[outputs]\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"System: 너는 사용자를 도와주는 훌륭한 AI 어시스턴트야.\\nHuman: what is 2.1^2.1\\nAI: \\nTool: Answer: 4.749638091742242\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > llm:ChatOpenAI] [958ms] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"The result of \\\\(2.1^{2.1}\\\\) is approximately 4.749638091742242.\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"model_name\": \"gpt-3.5-turbo-0125\",\n",
            "          \"service_tier\": \"default\"\n",
            "        },\n",
            "        \"type\": \"ChatGenerationChunk\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessageChunk\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"The result of \\\\(2.1^{2.1}\\\\) is approximately 4.749638091742242.\",\n",
            "            \"response_metadata\": {\n",
            "              \"finish_reason\": \"stop\",\n",
            "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
            "              \"service_tier\": \"default\"\n",
            "            },\n",
            "            \"type\": \"AIMessageChunk\",\n",
            "            \"id\": \"run--832b48e8-8202-4b1f-839a-b059e16a03c5\",\n",
            "            \"tool_calls\": [],\n",
            "            \"invalid_tool_calls\": []\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": null,\n",
            "  \"run\": null,\n",
            "  \"type\": \"LLMResult\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > parser:ToolsAgentOutputParser] Entering Parser run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > parser:ToolsAgentOutputParser] [1ms] Exiting Parser run with output:\n",
            "\u001b[0m[outputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence] [987ms] Exiting Chain run with output:\n",
            "\u001b[0m[outputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor] [2.56s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"The result of \\\\(2.1^{2.1}\\\\) is approximately 4.749638091742242.\"\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'what is 2.1^2.1',\n",
              " 'output': 'The result of \\\\(2.1^{2.1}\\\\) is approximately 4.749638091742242.'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 툴을 생성하는 방법\n",
        "\n",
        "- 1. tool 데코레이터 사용하기\n",
        "- 2. StructuredTool 이라는 클래스를 사용(복잡)"
      ],
      "metadata": {
        "id": "wI-BkSPkFnAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tool 데코레이터를 사용해서 일반 함수를 '툴'로 인지시키기 위함\n",
        "from langchain.tools import tool\n",
        "\n",
        "# @(at) 파이썬에서는 얘를 데코레이터라고 함\n",
        "# 함수, 클래스에 특정한 '기능을 추가하겠다'라는 약속\n",
        "# @app.route() / @staticmethod\n",
        "@tool\n",
        "def write_mail(to:str, subject:str, content:str) -> str:\n",
        "    \"\"\"Write and Send an email\"\"\" # 툴에 대한 설명을 반드시 해줘야 함\n",
        "    return f\"Email sent to {to} with subject {subject}, and {content}\"\n",
        "\n",
        "type(write_mail)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "MSflYwCyCm6D",
        "outputId": "a078fba6-bb55-433b-ad21-b6a0c3ae456d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.tools.structured.StructuredTool"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.tools.structured.StructuredTool</b><br/>def warning_emitting_wrapper(*args: Any, **kwargs: Any) -&gt; Any</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/langchain_core/tools/structured.py</a>Tool that can operate on any number of inputs.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 39);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#만든 툴을 사용하자!\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "#다양한 api를 넘나들며 쓸 수 있다.\n",
        "llm = init_chat_model('openai:gpt-4.1', temperature=0)\n",
        "\n",
        "#llm에 tools을 바인딩해줌\n",
        "#tool_choice = 'any' llm이 필요한 도구(tool)을 자유롭게 선택해서 활용하세요. //none(툴 사용 X), auto(자동 판단)\n",
        "#parallel_tool_calls = 에이전트가 한번에 '한개'/'여러개' 의 도구를 사용할 수 있게 함\n",
        "model_with_tools = llm.bind_tools([write_mail],\n",
        "                                  tool_choice='any',\n",
        "                                  parallel_tool_calls=False)\n",
        "\n",
        "output = model_with_tools.invoke(\"Draft a response to my coworker(jeongeunswd@gmail.com) about today's LangGraph lecture\")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wheyUYuFHjd_",
        "outputId": "96b51fab-f54c-46c9-81a1-876841373fe5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='' additional_kwargs={'tool_calls': [{'id': 'call_q75Sa29XckUVsaqgR8jFnWfg', 'function': {'arguments': '{\"to\":\"jeongeunswd@gmail.com\",\"subject\":\"Re: Today\\'s LangGraph Lecture\",\"content\":\"Hi Jeongeun,\\\\n\\\\nThank you for reaching out about today\\'s LangGraph lecture. I found the session quite insightful, especially the parts about graph-based workflow orchestration and how LangGraph integrates with LLMs. If you have any specific questions or want to discuss any of the topics further, let me know! I\\'d be happy to chat or share my notes.\\\\n\\\\nLooking forward to hearing your thoughts on the lecture.\\\\n\\\\nBest,\\\\n[Your Name]\"}', 'name': 'write_mail'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 126, 'prompt_tokens': 71, 'total_tokens': 197, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_3502f4eb73', 'id': 'chatcmpl-C8IonqZoSBxXlAMuiLS9HXsdO2d6K', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None} id='run--d833e30c-bd27-4574-a5f8-669d08ace106-0' tool_calls=[{'name': 'write_mail', 'args': {'to': 'jeongeunswd@gmail.com', 'subject': \"Re: Today's LangGraph Lecture\", 'content': \"Hi Jeongeun,\\n\\nThank you for reaching out about today's LangGraph lecture. I found the session quite insightful, especially the parts about graph-based workflow orchestration and how LangGraph integrates with LLMs. If you have any specific questions or want to discuss any of the topics further, let me know! I'd be happy to chat or share my notes.\\n\\nLooking forward to hearing your thoughts on the lecture.\\n\\nBest,\\n[Your Name]\"}, 'id': 'call_q75Sa29XckUVsaqgR8jFnWfg', 'type': 'tool_call'}] usage_metadata={'input_tokens': 71, 'output_tokens': 126, 'total_tokens': 197, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output.tool_calls[0]['args']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piNCMsLKaf2b",
        "outputId": "be26fa74-b653-4140-bfd8-50994e1b15c0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'to': 'jeongeunswd@gmail.com',\n",
              " 'subject': \"Re: Today's LangGraph Lecture\",\n",
              " 'content': \"Hi Jeongeun,\\n\\nThank you for reaching out about today's LangGraph lecture. I found the session quite insightful, especially the parts about graph-based workflow orchestration and how LangGraph integrates with LLMs. If you have any specific questions or want to discuss any of the topics further, let me know! I'd be happy to chat or share my notes.\\n\\nLooking forward to hearing your thoughts on the lecture.\\n\\nBest,\\n[Your Name]\"}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = write_mail(output.tool_calls[0]['args'])\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgcFhwMmbNch",
        "outputId": "1dda6ad4-4892-4371-ca29-aa30a5227852"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Email sent to jeongeunswd@gmail.com with subject Re: Today's LangGraph Lecture, and Hi Jeongeun,\n",
            "\n",
            "Thank you for reaching out about today's LangGraph lecture. I found the session quite insightful, especially the parts about graph-based workflow orchestration and how LangGraph integrates with LLMs. If you have any specific questions or want to discuss any of the topics further, let me know! I'd be happy to chat or share my notes.\n",
            "\n",
            "Looking forward to hearing your thoughts on the lecture.\n",
            "\n",
            "Best,\n",
            "[Your Name]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2047010431.py:1: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result = write_mail(output.tool_calls[0]['args'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 랭그래프 구성 요소\n",
        "\n",
        "- agent, tool => '기능'\n",
        "- 노드(Nodes) : 정보를 어떻게 다룰 것인가(액션)\n",
        "- 엣지(Edges) : 이 노드를 어떤 순서로 연결할 것인가 (흐름)\n",
        "- 상태(State) : 랭그래프의 흐름에서 추적해야할 정보의 상태"
      ],
      "metadata": {
        "id": "aPAhkIrAcMvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://langchain-ai.github.io/langgraph/concepts/why-langgraph/\n",
        "# 1. 기본적인 챗봇을 만듦(흐름)\n",
        "# 2. 툴을 추가한다.\n",
        "# 3. 메모리를 추가한다.\n",
        "# 4. 사람 피드백을 추가한다.\n",
        "# 5. 상태(State)를 커스터마이징 한다."
      ],
      "metadata": {
        "id": "sgfoyDEcbpAp"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 상태(State)에 대한 정의\n",
        "# fastapi로 딥러닝 추론 코드 => typing/pydantic 자료형 고정시킴\n",
        "# typing/pydantic -> TypedDict가 상태에 들어갈 구성요소의 '자료형'을 고정\n",
        "from typing import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "# 상태를 클래스로 만듦\n",
        "class StateSchema(TypedDict):\n",
        "    request : str\n",
        "    email : str\n",
        "\n",
        "# 나의 전체 LangGraph를 'workflow'라고 부를 것임\n",
        "# 그리고 workflow는 StateSchema라는 클래스를 상태로 가질 것임\n",
        "workflow = StateGraph(StateSchema)\n"
      ],
      "metadata": {
        "id": "kmKkhgeSeneq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 노드 만들기\n",
        "# 매개변수, 리턴값으로 State를 컨트롤하는 클래스를 받는다.\n",
        "def write_email_node(state : StateSchema) -> StateSchema:\n",
        "    # model_with_tools -> write_mail이라는 함수를 tool로 가진 llm\n",
        "    output = model_with_tools.invoke(state['request'])\n",
        "\n",
        "    # args 안에는 to, subject, content\n",
        "    args = output.tool_calls[0]['args']\n",
        "\n",
        "    # email -> 내가 요청한(request) 내용을 반영한 메일 초안\n",
        "    email = write_mail.invoke(args)\n",
        "    return {'email':email}"
      ],
      "metadata": {
        "id": "3WV6AX0lgTeB"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 노드와 엣지를 '등록', '연결'\n",
        "# add_node -> 함수를 노드로 인식하게 등록, (노드이름, 함수이름)\n",
        "workflow.add_node('write_email',write_email_node)\n",
        "\n",
        "# START -> 노드\n",
        "# 노드 -> END\n",
        "workflow.add_edge(START, 'write_email')\n",
        "workflow.add_edge('write_email', END)\n",
        "\n",
        "# 빌드 /컴파일\n",
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "Gc_-tClkiWOG"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "  #우리가 만든 app의 graph를 이미지로 그려서 display()로 감싸 출력하는 함수\n",
        "  display(Image(app.get_graph().draw_mermaid_png()))\n",
        "except Exception:\n",
        "  pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "em5LBEJ9ji_y",
        "outputId": "a5265b0b-7b9a-4b96-f3bf-ec1ea19fbd2c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# START -> LLM -> (조건 판단) -> tool -> END\n",
        "#                             -> ------> END\n",
        "from typing import Literal\n",
        "from langgraph.graph import MessagesState   # 위에 예제의 'StateGraph'역할을 대신 수행\n",
        "\n",
        "# 1. LLM 노드를 구성할 함수 만들기\n",
        "def call_LLM(state : MessagesState) -> MessagesState:\n",
        "    \"\"\"Run LLM\"\"\" # doctring(노드의 역할 설명)\n",
        "    output = model_with_tools.invoke(state['messages'])\n",
        "    return {'messages':[output]}\n",
        "\n",
        "# def write_email_node(state : StateSchema) -> StateSchema:\n",
        "#     # model_with_tools -> write_mail이라는 함수를 tool로 가진 llm\n",
        "#     output = model_with_tools.invoke(state['request'])\n",
        "\n",
        "#     # args 안에는 to, subject, content\n",
        "#     args = output.tool_calls[0]['args']\n",
        "\n",
        "#     # email -> 내가 요청한(request) 내용을 반영한 메일 초안\n",
        "#     email = write_mail.invoke(args)\n",
        "#     return {'email':email}\n",
        "\n",
        "# 2. tool을 사용하는 노드를 구성하는 함수 만들기\n",
        "def call_Tool(state : MessagesState) -> MessagesState:\n",
        "    \"\"\"Play the tool function\"\"\"\n",
        "    # 툴을 썼을 때, 툴에 to, subject, content라는 요소/tool을 써서 어떻게 했다~ 라는 결과를 남겨놓 기 위해\n",
        "    result = []\n",
        "    # state['messages'][-1] == output\n",
        "    for tool in state['messages'][-1].tool_calls:\n",
        "        observations = write_mail.invoke(tool['args'])\n",
        "        result.append({'role':'tool', 'content':observations, 'tool_call_id':tool['id']})\n",
        "    return {'messages':result}"
      ],
      "metadata": {
        "id": "LoCJxT3Zjq-M"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# START -> LLM -> (조건 판단) -> tool -> END\n",
        "#                             -> ------> END\n",
        "# 조건분기 판단 **리턴값이 특이함\n",
        "# __init__ : 던더 함수(매직 메서드)\n",
        "# END = __end__\n",
        "def flow_check(state:MessagesState) -> Literal['call_Tool', '__end__']:\n",
        "    \"\"\"Route to tool handling, or end if don't need tool call\"\"\"\n",
        "    message = state['messages']\n",
        "    latest = message[-1]    # 상태 그래프가 가지고 있는 가장 최신의 정보를 get\n",
        "\n",
        "    # 'tool을 call'했던 이력이 있냐 없냐? 있으면 -> call_tool, 없으면 -> END\n",
        "    if latest.tool_calls:\n",
        "        return 'call_Tool'\n",
        "    else:\n",
        "        return END"
      ],
      "metadata": {
        "id": "QPzy5c_nrU8H"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#노드를 순서대로 엮자!\n",
        "#1. 상태 그래프를 정의한다.\n",
        "workflow = StateGraph(MessagesState)\n",
        "\n",
        "#2. 노드를 등록한다.\n",
        "workflow.add_node('call_LLM', call_LLM)\n",
        "workflow.add_node('call_Tool', call_Tool)\n",
        "\n",
        "#3. 조건분기 등록\n",
        "#컨디셔널엣지(시작점, 분기함수, {결과1:결과노드, 결과2:결과노드})\n",
        "workflow.add_conditional_edges('call_LLM', flow_check, {'call_Tool':'call_Tool', END:END})\n",
        "\n",
        "#4. 나머지 엣지 연결\n",
        "workflow.add_edge(START, 'call_LLM')\n",
        "workflow.add_edge('call_Tool', END)\n",
        "\n",
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "xLtqLnmcuVW4"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = app.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Draft a response to my coworker(jeongeunswd@gmail) confirming that I want to attend Interrupt!\"}]})\n",
        "for m in result[\"messages\"]:\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GB9YAr4wSSg",
        "outputId": "90926d83-34ea-48d5-c05e-3f8d60715e12"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Draft a response to my coworker(jeongeunswd@gmail) confirming that I want to attend Interrupt!\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  write_mail (call_Agosg7FMbzo3JpiHJzxDHKTc)\n",
            " Call ID: call_Agosg7FMbzo3JpiHJzxDHKTc\n",
            "  Args:\n",
            "    to: jeongeunswd@gmail\n",
            "    subject: Re: Interrupt! Attendance\n",
            "    content: Hi,\n",
            "\n",
            "Thank you for letting me know about Interrupt! I’d like to confirm that I want to attend. Please let me know if there’s anything I need to prepare or any further details I should know.\n",
            "\n",
            "Looking forward to it!\n",
            "\n",
            "Best,\n",
            "[Your Name]\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "\n",
            "Email sent to jeongeunswd@gmail with subject Re: Interrupt! Attendance, and Hi,\n",
            "\n",
            "Thank you for letting me know about Interrupt! I’d like to confirm that I want to attend. Please let me know if there’s anything I need to prepare or any further details I should know.\n",
            "\n",
            "Looking forward to it!\n",
            "\n",
            "Best,\n",
            "[Your Name]\n"
          ]
        }
      ]
    }
  ]
}